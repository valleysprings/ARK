# LLM API Configuration
# API keys and base URLs are imported from credentials.yaml in the project root
# See credentials.yaml.example for the required structure
llm_api:
  # DeepSeek API
  deepseek:
    enabled: true
    model: "deepseek-chat"  # Correct model name
    timeout: 1200
    max_retries: 2

  # GPT API
  gpt:
    enabled: true
    model: "gemini-2.5-flash"  # Available: gpt-4o, gemini-2.5-flash-preview-04-17, etc.
    timeout: 1200
    max_retries: 2

  # Google Gemini API
  gemini:
    enabled: true
    model: "gemini-2.5-flash"
    timeout: 1200
    max_retries: 2

  # Ollama Local API
  ollama:
    enabled: true
    model: "mistral:latest"  # Available: mistral:latest, qwen2:7b, etc.
    timeout: 60

  # General settings
  settings:
    max_async_calls: 10  # Maximum concurrent API calls
    default_provider: "gpt"  # Default: gemini | deepseek | gpt | ollama
    enable_token_tracking: true
    retry_delay: 10  # Initial retry delay in seconds
