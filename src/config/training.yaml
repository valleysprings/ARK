# Training Configuration
# Configuration for fine-tuning retrieval models

# Paths configuration
paths:
  data_root: "./data"
  model_root: "./model"
  output_root: "./experiments/results"
  cache_root: "./data/preprocessed"

# Training Configuration
training:
  # Common settings
  common:
    output_dir: "./model/checkpoints"
    logging_dir: "./experiments/logs"
    seed: 42
    fp16: true
    gradient_checkpointing: false

  # BGE-M3 Training
  bge:
    enabled: true
    model_name_or_path: "./model/bge-m3"

    # Training hyperparameters
    per_device_train_batch_size: 700
    per_device_eval_batch_size: 128
    gradient_accumulation_steps: 1
    learning_rate: 1.0e-5
    num_train_epochs: 5
    warmup_ratio: 0.1
    weight_decay: 0.01

    # Model-specific
    unified_finetuning: true  # Train dense/sparse/colbert jointly
    use_self_distill: true
    knowledge_distillation: true
    temperature: 0.7
    negatives_cross_device: true
    train_group_size: 8  # 1 positive + 7 negatives

    # Loss weights
    lambda_init: 0.9  # Initial weight for teacher loss
    lambda_final: 0.1  # Final weight for teacher loss
    lambda_schedule: "cosine"  # cosine | linear

  # Qwen Embedding Training
  qwen:
    enabled: true
    model_name_or_path: "Qwen/Qwen3-Embedding-0.6B"

    # Training hyperparameters
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 16
    learning_rate: 6.0e-6
    num_train_epochs: 10
    warmup_steps: 100
    weight_decay: 0.01

    # Triple alignment scoring
    alignment:
      forward_weight: 1.0    # P(answer|chunk, question)
      backward_weight: 0.3   # P(question|chunk, answer)
      parameter_weight: 1.0  # cosine(query_emb, chunk_emb)
      top_m: 10  # Number of positive chunks to select

    # Curriculum learning stages
    curriculum:
      stage1_epochs: 3  # Initial alignment
      stage2_epochs: 4  # Coarse-grained (large subgraph)
      stage3_epochs: 3  # Fine-grained (small subgraph)

    # Loss function
    loss_type: "cosine_similarity"  # For MS-SWIFT
    task_type: "embedding"

# Evaluation Configuration
evaluation:
  # Datasets
  datasets:
    - name: "hotpotqa"
      path: "./data/raw/hotpotqa.jsonl"
      metric: "f1"
    - name: "2wikimqa"
      path: "./data/raw/2wikimqa.jsonl"
      metric: "f1"
    - name: "narrativeqa"
      path: "./data/raw/narrativeqa.jsonl"
      metric: "f1"
    - name: "musique"
      path: "./data/raw/musique.jsonl"
      metric: "f1"
    - name: "qasper"
      path: "./data/raw/qasper.jsonl"
      metric: "f1"

  # Metrics
  metrics:
    - "f1"
    - "exact_match"
    - "rouge"

  # Evaluation settings
  batch_size: 16
  max_samples: -1  # -1 for all samples
  save_predictions: true

# Logging and Monitoring
logging:
  level: "INFO"  # DEBUG | INFO | WARNING | ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./experiments/logs/ark.log"
  console: true

# Experiment Management
experiment:
  name: "default"
  tags: []
  notes: ""
  save_config: true
  save_code: false
  use_wandb: false
  wandb_project: "ark"
  wandb_entity: null
